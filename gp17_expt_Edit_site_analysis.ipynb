{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Import\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import fuzzysearch\n",
    "import matplotlib as plt\n",
    "import itertools\n",
    "import pickle\n",
    "import demultiplexing_module\n",
    "import gzip\n",
    "plt.rcParams['pdf.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load key of conditions and files\n",
    "samples = pd.read_excel(\"gp17_expt_file_key.xlsx\")[[\"condition\",\"run\",\"file\",\"site1_L\",\"site1_R\",\"site1_wt\",\"site1_pos\",\"site1_neut\",\"site2_L\",\"site2_R\",\"site2_wt\",\"site2_pos\",\"site2_neut\",\"site3_L\",\"site3_R\",\"site3_wt\",\"site3_pos\",\"site3_neut\",\"site4_L\",\"site4_R\",\"site4_wt\",\"site4_pos\",\"site4_neut\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Globals\n",
    "outcomes_dict = {} #editing per site\n",
    "outcomes_dict_complete = {} #matched outcomes across a site\n",
    "outcomes_dict_select = {} #no - outcomes\n",
    "site_outcomes = 'wpn-/' #wild-type, positive, neutral, neither, no site\n",
    "fewer_outcomes = 'wpn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Defs\n",
    "def extract_and_match(read,index,rep):\n",
    "    across_sites = []\n",
    "    for site in range(1,5):\n",
    "        if site == 1 : #these sites don't have any other near matches and seem to work easily\n",
    "            left_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_L\" % site],read,max_l_dist=1,max_deletions=0,max_insertions=0,max_substitutions=1)\n",
    "            right_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_R\" % site],read,max_l_dist=1,max_deletions=0,max_insertions=0,max_substitutions=1)\n",
    "        if site == 4: #these sites don't have any other near matches and seem to work easily\n",
    "            left_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_L\" % site],read,max_l_dist=2,max_deletions=0,max_insertions=0,max_substitutions=2)\n",
    "            right_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_R\" % site],read,max_l_dist=2,max_deletions=0,max_insertions=0,max_substitutions=2)\n",
    "        if site == 2: #left side has a near match, so making it longer and allowing more mismatches\n",
    "            left_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_L\" % site],read,max_l_dist=2,max_deletions=0,max_insertions=0,max_substitutions=2)\n",
    "            right_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_R\" % site],read,max_l_dist=4,max_deletions=0,max_insertions=0,max_substitutions=4) \n",
    "        if site == 3:\n",
    "            left_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_L\" % site],read,max_l_dist=3,max_deletions=0,max_insertions=0,max_substitutions=3)\n",
    "            right_inside = fuzzysearch.find_near_matches(samples.iloc[index][\"site%s_R\" % site],read,max_l_dist=3,max_deletions=0,max_insertions=0,max_substitutions=3)             \n",
    "        if len(left_inside) == 1 and len(right_inside) == 1:\n",
    "            var_nt = read[left_inside[0].end:right_inside[0].start]\n",
    "            if var_nt == samples.iloc[index][\"site%s_wt\" % site]:\n",
    "                across_sites.append('w')\n",
    "            elif var_nt == samples.iloc[index][\"site%s_pos\" % site]:\n",
    "                across_sites.append('p')\n",
    "            elif var_nt == samples.iloc[index][\"site%s_neut\" % site]:\n",
    "                across_sites.append('n')\n",
    "            else:\n",
    "                across_sites.append('-')\n",
    "        else:\n",
    "            across_sites.append('/')\n",
    "    return tuple(across_sites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#step through samples\n",
    "for i in samples.index:\n",
    "    sample_i = int(i)\n",
    "    outcomes_dict[samples['file'][i]] = {}\n",
    "    outcomes_dict_complete[samples['file'][i]] = {}\n",
    "    outcomes_dict[samples['file'][i]] = {}\n",
    "    outcomes_dict_complete[samples['file'][i]] = {}\n",
    "    outcomes_dict_select[samples['file'][i]] = {}\n",
    "    for product in itertools.product(site_outcomes, repeat=4):\n",
    "         outcomes_dict_complete[samples['file'][i]][product] = 0\n",
    "    for product in itertools.product(fewer_outcomes, repeat=4):\n",
    "         outcomes_dict_select[samples['file'][i]][product] = 0\n",
    "    for site in range(1,5):\n",
    "        outcomes_dict[samples['file'][i]][site] = {'wt':0,'pos_ed':0,'neut_ed':0,'neither':0,'nosite':0}\n",
    "    all_reads_str = []\n",
    "    read_counter = []\n",
    "#     fastq_reads = \"./%s_R1.fastq\" % samples['file'][i]\n",
    "    fastq_reads = demultiplexing_module.get_file_path(samples['run'][i],samples['file'][i])\n",
    "    print (fastq_reads[1])\n",
    "    try:\n",
    "#         for seq_record in SeqIO.parse(fastq_reads, \"fastq\"):\n",
    "        with gzip.open(fastq_reads[1], \"rt\") as handle: \n",
    "            for seq_record in SeqIO.parse(handle, \"fastq\"):\n",
    "                all_reads_str.append(str(seq_record.seq))\n",
    "            read_counter = Counter(all_reads_str)\n",
    "            for read in read_counter:\n",
    "                outcomes = extract_and_match(read,i,samples['file'])\n",
    "                outcomes_dict_complete[samples['file'][i]][outcomes] += read_counter[read]\n",
    "                try: outcomes_dict_select[samples['file'][i]][outcomes] += read_counter[read]\n",
    "                except KeyError: pass     \n",
    "                for position,outcome in enumerate(outcomes):\n",
    "                    if outcome == 'w':\n",
    "                        outcomes_dict[samples['file'][i]][position+1]['wt'] += read_counter[read]\n",
    "                    elif outcome == 'p':\n",
    "                        outcomes_dict[samples['file'][i]][position+1]['pos_ed'] += read_counter[read]\n",
    "                    elif outcome == 'n':\n",
    "                        outcomes_dict[samples['file'][i]][position+1]['neut_ed'] += read_counter[read] \n",
    "                    elif outcome == '-':\n",
    "                        outcomes_dict[samples['file'][i]][position+1]['neither'] += read_counter[read]\n",
    "                    elif outcome == '/':\n",
    "                        outcomes_dict[samples['file'][i]][position+1]['nosite'] += read_counter[read]\n",
    "            print(samples['file'][i])\n",
    "    except IOError: #this happens when a file is missing\n",
    "        print(\"%s missing\" % samples['file'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##calculate summary data\n",
    "Ed_Per = {}\n",
    "No_Site_Per = {}\n",
    "Total_Count = {}\n",
    "Neither_Per = {}\n",
    "\n",
    "Ed_Per['Site1_pos'] = []\n",
    "Ed_Per['Site2_pos'] = []\n",
    "Ed_Per['Site3_pos'] = []\n",
    "Ed_Per['Site4_pos'] = []\n",
    "\n",
    "Ed_Per['Site1_neut'] = []\n",
    "Ed_Per['Site2_neut'] = []\n",
    "Ed_Per['Site3_neut'] = []\n",
    "Ed_Per['Site4_neut'] = []\n",
    "\n",
    "Neither_Per['Site1'] = []\n",
    "Neither_Per['Site2'] = []\n",
    "Neither_Per['Site3'] = []\n",
    "Neither_Per['Site4'] = []\n",
    "\n",
    "No_Site_Per['Site1'] = []\n",
    "No_Site_Per['Site2'] = []\n",
    "No_Site_Per['Site3'] = []\n",
    "No_Site_Per['Site4'] = []\n",
    "\n",
    "Total_Count['Site1'] = []\n",
    "Total_Count['Site2'] = []\n",
    "Total_Count['Site3'] = []\n",
    "Total_Count['Site4'] = []\n",
    "\n",
    "for i in samples.index:\n",
    "    for site in range(1,5):\n",
    "        try:\n",
    "            Ed_Per['Site%s_pos' % site].append((float(outcomes_dict[samples['file'][i]][site]['pos_ed']) / (outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt']))*100)\n",
    "        except ZeroDivisionError:\n",
    "            Ed_Per['Site%s_pos' % site].append(\"div0\")\n",
    "        try:\n",
    "            Ed_Per['Site%s_neut' % site].append((float(outcomes_dict[samples['file'][i]][site]['neut_ed']) / (outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt']))*100)\n",
    "        except ZeroDivisionError:\n",
    "            Ed_Per['Site%s_neut' % site].append(\"div0\")\n",
    "        try:\n",
    "            Neither_Per['Site%s' % site].append((float(outcomes_dict[samples['file'][i]][site]['neither']) / (outcomes_dict[samples['file'][i]][site]['neither']+outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt']))*100)\n",
    "        except ZeroDivisionError:\n",
    "            Neither_Per['Site%s' % site].append(\"div0\")   \n",
    "        try:\n",
    "            No_Site_Per['Site%s' % site].append((float(outcomes_dict[samples['file'][i]][site]['nosite']) / (outcomes_dict[samples['file'][i]][site]['nosite']+outcomes_dict[samples['file'][i]][site]['neither']+outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt']))*100)\n",
    "        except ZeroDivisionError:\n",
    "            No_Site_Per['Site%s' % site].append(\"div0\")  \n",
    "        #total count per site excludes the nosite counts where the flanking sequences were not identified\n",
    "        Total_Count['Site%s' % site].append(outcomes_dict[samples['file'][i]][site]['neither']+outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in samples.index:\n",
    "    for site in range(1,5):\n",
    "        print (samples['condition'][i], 'site %s' % site)\n",
    "        print ('number of missing sites: %s' % outcomes_dict[samples['file'][i]][site]['nosite'])\n",
    "        print (outcomes_dict[samples['file'][i]][site]['neither'],outcomes_dict[samples['file'][i]][site]['pos_ed'],outcomes_dict[samples['file'][i]][site]['neut_ed'],outcomes_dict[samples['file'][i]][site]['wt'])\n",
    "        print ('total seqs analyzed: %s' % (outcomes_dict[samples['file'][i]][site]['neither']+outcomes_dict[samples['file'][i]][site]['pos_ed']+outcomes_dict[samples['file'][i]][site]['neut_ed']+outcomes_dict[samples['file'][i]][site]['wt']))\n",
    "        print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples.insert(2,'Site1_Pos_Edit_Percent',Ed_Per['Site1_pos'])\n",
    "samples.insert(3,'Site2_Pos_Edit_Percent',Ed_Per['Site2_pos'])\n",
    "samples.insert(4,'Site3_Pos_Edit_Percent',Ed_Per['Site3_pos'])\n",
    "samples.insert(5,'Site4_Pos_Edit_Percent',Ed_Per['Site4_pos'])\n",
    "\n",
    "samples.insert(6,'Site1_Neut_Edit_Percent',Ed_Per['Site1_neut'])\n",
    "samples.insert(7,'Site2_Neut_Edit_Percent',Ed_Per['Site2_neut'])\n",
    "samples.insert(8,'Site3_Neut_Edit_Percent',Ed_Per['Site3_neut'])\n",
    "samples.insert(9,'Site4_Neut_Edit_Percent',Ed_Per['Site4_neut'])\n",
    "\n",
    "samples.insert(10,'Site1_NeitherNonWT_Percent',Neither_Per['Site1'])\n",
    "samples.insert(11,'Site2_NeitherNonWT_Percent',Neither_Per['Site2'])\n",
    "samples.insert(12,'Site3_NeitherNonWT_Percent',Neither_Per['Site3'])\n",
    "samples.insert(13,'Site4_NeitherNonWT_Percent',Neither_Per['Site4'])\n",
    "\n",
    "samples.insert(14,'Site1_Missing_Percent',No_Site_Per['Site1'])\n",
    "samples.insert(15,'Site2_Missing_Percent',No_Site_Per['Site2'])\n",
    "samples.insert(16,'Site3_Missing_Percent',No_Site_Per['Site3'])\n",
    "samples.insert(17,'Site4_Missing_Percent',No_Site_Per['Site4'])\n",
    "\n",
    "samples.insert(18,'Site1_Total_Count',Total_Count['Site1'])\n",
    "samples.insert(19,'Site2_Total_Count',Total_Count['Site2'])\n",
    "samples.insert(20,'Site3_Total_Count',Total_Count['Site3'])\n",
    "samples.insert(21,'Site4_Total_Count',Total_Count['Site4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##output\n",
    "samples.to_excel(\"expt_editing_analysis_output.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes_dict_all_sites_seq = {} #removes the '-' or '/' containing sequences (where one or more sites isn't cleanly sequenced)\n",
    "\n",
    "for sample_key in outcomes_dict_complete:\n",
    "    outcomes_dict_all_sites_seq[sample_key] = {k:v for k,v in outcomes_dict_complete[sample_key].items() if '-' not in k and '/' not in k}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn outcomes dict into percent\n",
    "outcomes_dict_all_sites_seq_percent = {}\n",
    "\n",
    "for sample_key in outcomes_dict_all_sites_seq:\n",
    "    outcomes_dict_all_sites_seq_percent[sample_key] = {}\n",
    "    for outcome in outcomes_dict_all_sites_seq[sample_key]:\n",
    "        outcomes_dict_all_sites_seq_percent[sample_key][outcome] = 100*(float(outcomes_dict_all_sites_seq[sample_key][outcome])/sum(outcomes_dict_all_sites_seq[sample_key].values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle the dict\n",
    "pickle.dump(outcomes_dict_all_sites_seq_percent,open(\"pickle_output.p\",\"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
